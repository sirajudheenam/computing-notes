OpenHPI DataScience Course


0 XP
Data Science Bootcamp
Mohamed Elhayany, Hendrik Steinbeck
Self-paced course
Learnings
Discussions
Progress
Collab Space
Course Details
Announcements
Recap

Your existing knowledge
Time effort: approx. 26 minutes
Please try the following question about various topics, that we have prepared in this course. You can submit this questionnaire only once, without any time constraints. Just answer the questions to the best of your existing knowledge, without any additional help.

The result will not be part of your grade – in fact it is not graded at all. This helps us to calibrate the content and find out more about your learning journey.

Question 1
What stages are usually part of the data science process?

Renting an office for developer and implementing an agile framework.
Analyzing a specific question with chosen tools on data to gain further insights.
Trying to apply as many models to a dataset as possible.
Finding the models that yield the highest fit to the underlying question asked by the business department.
Question 2
What role does ethics place in the context of data science? Note: Multiple answers are correct.

No role at all. Computer are coherent and can translate all-day cases very easily.
It can be used to find out where a dataset comes from and whether it should be used at all.
If an underlying bias is part of a model, these consideration can be raised during discovery or by testing it as a separate hypothesis.
Question 3
Mark the correct datatype allocation. Note: Multiple answers are correct.

Name of the favorite lunch – integer
Double – Birthday in the format DDMMYYYY
Economy, Business and First Class category as an integer
String – favorite lunch
Height in centimeters as a float or double
Vegetarian or not as a float
Question 4
What is the purpose of data visualization?

To analyze data using statistical methods.
To create predictive models based on historical data.
To collect and store large amounts of data.
To make data easier to understand and interpret.
Question 5
Which type of data visualization is best suited for showing the distribution of a single numerical variable?

Scatter plot
Bar chart
Box plot
Bubble chart
Radar chart
Piano chart
Question 6
What statement is true about SQL?

Relational databases are the best way to model a scenario.
It is better to work directly on databases, instead of a representation or copy.
Various approaches and libraries exists to use SQL in various environments.
There is only one programming implementation of SQL.
Question 7
Which SQL queries are correct? Note: Multiple answers are correct.

SELECT column1.table1, column1.table2 FROM table1, table2 WHERE key.table1 = key.table2
SELECT * FROM table1 WHERE columnName="searchTeam" OR columnName2 ="otherTerm"
SELECT table1.column2, table2.column1 FROM table1, table2 WHERE table1.key = table2.key
SELECT * FROM table1, table2 WHERE columnName="searchTeam"
SELECT * FROM table1 WHERE columnName="searchTeam"
PRINT * from table1 OR table2
Question 8
What is the goal of an exploratory data analysis?

The step of an EDA can be skipped entirely, as it is an auxiliary step.
Increasing the KPIs of a model early in the building process.
Getting a first insight into the data and its distribution.
Understanding the dataset completely
Question 9
How many variable(s) are meant to be analyzed via an univariate analysis?

1
2
3
4
Question 10
What are typical statistical metrics applied in a univariate analysis?

Calculate Average, Median and Standard Deviation.
Simulate the most likely results by an Monte-Carlo simulation.
Accepting a hypothesis through an t-test with p>0.05
Question 11
What is the goal of bivariate analysis?

To understand the relationship between two variables.
To analyze data using advanced statistical techniques.
To create predictive models based on historical data.
To visualize data in multiple dimensions.
All of the above
Question 12
Which type of data visualization is commonly used to display the relationship between two numerical variables?

Scatter plot
Bar chart
Heat map
Line graph
Radar chart
Question 13
Is this statement true? Various models exists to conduct a multivariate analysis (MVA) based on different scenarios.

True
False
Question 14
What else can be said about multivariate analysis? Note: Multiple answers are correct.

MVA is a rather complicated model, but it allows to include a wide range of variables.
Only numerical data can be used.
The same input variables can be used to calculate different output variables.
It explains how much the input variables impacts the output.
Only categorical and ordinal data can be used.
One set of input variables can only have one output variable.
Question 15
What is the purpose of formatting data in the context of data analysis?

To visualize data using charts and graphs.
To ensure data consistency and uniformity.
To perform statistical calculations on the data.
To extract insights and patterns from the data.
Question 16
Which data normalization technique is commonly used to scale numerical data between a specified range, such as 0 and 1?

Log transformation
Standardization
Z-score normalization
Min-max normalization
Question 17
What is the primary goal of machine learning?

To create algorithms for solving complex mathematical problems.
To process and analyze large amounts of data quickly.
To develop intelligent machines capable of learning from data.
To design efficient computer systems.
Question 18
What does the following code snippet achieve?

X = df.drop('category', axis=1)  
y = df['category']
Duplicates y, deletes X
None of the above
Splits the dataset
Deletes the x-axis of the dataframe
Question 19
What is a label?

A clustering technique applied in the EDA phase.
The identified data that is used in training.
Used memory in the header-function of the logistical regression.
The known bias of a dataset, published by previous users.
Question 20
What is a typical split-ratio between the labeled training and testing data?

40 training : 60 testing
50 : 50
80 : 20
20 : 80
30 : 70
Any of the above
Question 21
Which technique is commonly used to address overfitting in machine learning?

Gradient boosting
Regularization
Principal component analysis (PCA)
Random forest
Question 22
What is the main objective of a linear regression model?

To classify data into different categories.
To estimate the probability of an event occurring.
To predict a numerical value based on input variables.
To analyze the relationship between two categorical variables.
Question 23
Which assumption is violated if the residuals of a linear regression model exhibit a nonlinear pattern?

Homoscedasticity
Normality
Independence
Linearity
Question 24
What is label encoding?

Pre-processing the labels to increase the model velocity.
A manual check of another team-member to make sure your logistic regression works.
Transforming a variable into a suitable format.
Compressing the size of a lot of 0 and 1 to leverage larger data sets.
The second step of the cleaning data procedure.
Question 25
What is true about decision trees? Note: Multiple answers are correct.

Decision trees are immune to overfitting, even with large and complex datasets.
If a decision trees predicts numerical values, it is called a regression tree.
Two main branches are considered a sub-tree.
A decision tree usually has a true and a false branch, leading to the next node.
The class nodes hold the labels of a given variable.
Decision Trees could be used for regression and classification problems alike.
Question 26
To estimate the error of a regression model, which scoring method is the appropriate one to use?

Mean Squared Error
Accuracy
Markov-Score
Average
Question 27
Which distance metric is commonly used in the KNN algorithm?

Saint Diego distance
None of the above
Greenwich points
Euclidean distance
Question 28
What is true about the size of the 'k' in a classification scenario? Note: Multiple answer are correct.

If k is too small, a lot of neighbors will be not included, making the classification not exact.
A KNN model is efficient at smaller sample sizes and becomes costly at larger sample sizes due to its memory-based learning characteristics.
K is not as significant, as we can influence the N-N part of the model much easier.
If k is too small, a lot of neighbors will be included, making the classification not exact.
If k is too large, a lot of neighbors will be included, making the classification not exact.
If k is too large, a lot of neighbors will be not be included, making the classification not exact.
Send feedback
‹
Previous

10 Questions for the Teaching Team | Q&A
Next

Recommendation for the course: Create your own code summary
›
Hide navigation
Overview
Introduction - Course Overview and Housekeeping
Welcome to the Bootcamp! Important information
Your existing knowledge
Introduction to JupyterLab
Discussions
Week 1 (Intro to Data Science and Visualization)
Week 2 (EDA and Statistical Analysis)
Week 3 (Machine Learning)
Week 4 (Final Exam 28.6 - 4.7)
PROJECT track & Peer Review
I like, I wish
More information
Course Program
About openHPI
Team
Publications
About MOOCs
openHPI at School
Code of Honor
Forum Rules
Open Source
Help
FAQ
Certificate Guidelines
Peer Assessment
Gamification
Certificate
Helpdesk
Contact
Follow us
Twitter
Facebook
GitHub
LinkedIn
YouTube
About openHPI
openHPI is the digital education platform of the Hasso Plattner Institute, Potsdam, Germany. On openHPI you take part in a worldwide social learning network based on interactive online courses covering different subjects in Information and Communication Technology (ICT).


© 2012 - 2023 Hasso Plattner Institute – ImprintData ProtectionPowered by HPI (r8334)


Overview of the Data Science life-cycle
Time effort: approx. 3 minutes
Introduction:
Data science has emerged as a powerful discipline that leverages advanced techniques to extract valuable insights and drive informed decision-making. To achieve this, data scientists follow a structured process known as the data science life cycle. This article provides an overview of the different stages involved in the data science life cycle and highlights their significance in transforming raw data into actionable insights.

Problem Definition:
Every data science project begins with clearly defining the problem at hand. This stage involves understanding the business objectives, identifying the key questions to be answered, and defining measurable goals. Effective problem definition lays the foundation for the subsequent stages of the data science life cycle.

Data Acquisition and Understanding:
Once the problem is defined, the next step is to gather relevant data from various sources. This may involve collecting data from databases, APIs, or external datasets. Understanding the data is crucial at this stage, including its structure, format, and quality. Data exploration techniques such as descriptive statistics and visualization aid in gaining insights and identifying potential challenges or data limitations.

Data Preparation and Cleaning:
Data in its raw form often contains errors, missing values, or inconsistencies. Data preparation involves cleaning and transforming the data to ensure it is suitable for analysis. This includes handling missing data, removing outliers, normalizing variables, and transforming data types. Additionally, data integration and feature engineering techniques are applied to create meaningful features that capture relevant information for analysis.

Exploratory Data Analysis (EDA):
EDA is an essential step where data scientists examine the data in detail to identify patterns, relationships, and trends. Techniques such as statistical analysis, data visualization, and hypothesis testing are employed to gain insights into the data. EDA helps in uncovering hidden patterns, understanding the data distribution, and formulating initial hypotheses for further analysis.

Modeling and Algorithm Selection:
In this stage, data scientists select appropriate modeling techniques and algorithms to build predictive or descriptive models. This involves selecting algorithms based on the problem type (classification, regression, clustering, etc.) and the nature of the data. Model training and evaluation are performed using suitable performance metrics to ensure the model's effectiveness and generalizability.

Model Deployment and Integration:
Once the model is developed and evaluated, it needs to be deployed into a production environment. This stage involves integrating the model into existing systems or applications to automate predictions or support decision-making processes. Deployment also includes monitoring the model's performance, retraining it periodically, and ensuring its reliability and scalability.

Communication and Visualization of Results:
The final stage of the data science life cycle involves presenting the insights and findings to stakeholders in a clear and understandable manner. Data visualization techniques such as charts, graphs, and dashboards are employed to effectively communicate the results. Visualizations aid in conveying complex information, highlighting key trends, and facilitating data-driven decision-making across various domains.

Conclusion:
The data science life cycle provides a systematic framework for extracting value from data. From problem definition to communicating insights, each stage plays a crucial role in transforming raw data into actionable knowledge. By following this structured process, data scientists can navigate the complexities of data analysis, ultimately unlocking the power of data to drive innovation and informed decision-making in diverse industries.

Practical Example of Lying with Visualizations
Ploting Graph packages:
- seaborn
- matplotlib
- plotnine

plot.hist(randomlistX)
plot.scatter(randomlistX, randomlistY)

# Introduction to SQL

- Collect
- Wrangle
- Clean
- Organize
- Display

Steps:
- Access
- Combine Variable
- Representation of Data (x+y = z)
    - Pull data and copy them to a dataframe.

- Library of sqlite

- SQL
    - Passengers 
    - Airlines
    - Employees

    ```sql
    SELECT * FROM AIRCRAFTS;
    SELECT NAME FROM AIRLINES;
    SELECT columnName FROM tableName;

    SELECT table.column FROM tableName;
    SELECT * FROM AIRCRAFTS, PASSENGERS; # Gives cross product of tables
    ```
    # 10000 passengers with 10 AIRCRAFT could give you 100000 ENTRIES with the above SQL command.

    SELECT * FROM AIRCRAFTS, PASSENGERS WHERE Passenger.flightNumber = Aircraft.flightNumber

    # Primary and Foreign Key

    ## Consistency

```sql
    SELECT * FROM PASSENGERS WHERE PASSENGERS.lastName = "Jones" OR PASSENGERS.firstName = "Jones" 
```
## Cardinality: = Relationship

1 to 1
1 to n (many)
n (many) to - m (many)

```sql
    SELECT aircraft_ID, Max(capacity), carrier_type FROM AIRCRAFTS GROUP BY carrier_type ORDER BY capacity DESC.
```

```sql
SELECT * FROM sqlite_master WHERE type='table'
# Another command that could bootstrap your detective-database-work, is getting an overview of the columns of a specific tableName:

```

```sql
PRAGMA table_info(tableName) 
```